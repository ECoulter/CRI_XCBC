---
#OpenHPC release version
  openhpc_release_rpm: "https://github.com/openhpc/ohpc/releases/download/v1.3.GA/ohpc-release-1.3-1.el7.x86_64.rpm"
#The full list of available versions for CentOS can be generated via
# curl -s https://github.com/openhpc/ohpc/releases/ | grep rpm | grep -v sle | grep -v strong  | sed 's/.*="\(.*\)".*".*".*/\1/'
#
# Headnode Info
  public_interface: "enp11s0f0" # NIC that allows access to the public internet
  private_interface: "enp11s0f1" #NIC that allows access to compute nodes
  headnode_private_ip: "172.29.2.1"
  build_kernel_ver: '3.10.0-514.el7.x86_64' # `uname -r` at build time... for wwbootstrap
  hnas_private_ip: "172.29.1.250"

#login node info
  login_public_interface: "eth0"
  login_private_interface: "eth1"

#Private network Info
  private_network: "172.29.0.0"  
  private_network_mask: "22"
  private_network_long_netmask: "255.255.252.0"
  compute_ip_minimum: "172.29.2.10"
  compute_ip_maximum: "172.29.2.255"
  login_ip_minimum: "172.29.2.2"

#slurm.conf variables
  cluster_name: "campanile"
#  gres_types: "gpu"

#Stateful compute or not?
  stateful_nodes: false

#Node Config Vars - for stateful nodes
  sda1: "mountpoint=/boot:dev=sda1:type=ext3:size=500"
  sda2: "dev=sda2:type=swap:size=500"
  sda3: "mountpoint=/:dev=sda3:type=ext3:size=fill"

# GPU Node Vars
#  nvidia_driver_installer: "NVIDIA-Linux-x86_64-375.39.run"

# WW Template Names for wwmkchroot
  template_path: "/usr/libexec/warewulf/wwmkchroot/"
  compute_template: "compute-nodes"
  gpu_template: "gpu-nodes"  
  login_template: "login-nodes"  

# Chroot variables
  compute_chroot_loc: "/opt/ohpc/admin/images/{{ compute_chroot }}"
  compute_chroot: centos7.3-compute
  gpu_chroot_loc: "/opt/ohpc/admin/images/{{ gpu_chroot }}"
  gpu_chroot: centos7.3-gpu
  login_chroot_loc: "/opt/ohpc/admin/images/{{ login_chroot }}"
  login_chroot: centos7.3-login 

# Node Inventory method - automatic, or manual
  node_inventory_auto: true

#Node naming variables - no need to change
  compute_node_prefix: "compute-"
  num_compute_nodes: 2
  gpu_node_prefix: "gpu-compute-"
  num_gpu_nodes: 0
  login_node_prefix: "login-"
  num_login_nodes: 1

#Node Inventory - not in the Ansible inventory sense! Just for WW and Slurm config.
# Someday I will need a role that can run wwnodescan, and add nodes to this file! Probably horrifying practice.
# There is a real difference between building from scratch, and using these for maintenance / node addition!
#
  compute_private_nic: "eth0"
  compute_nodes:
   - { name: "compute-1", vnfs: '{{compute_chroot}}',  cpus: 1, sockets: 1, corespersocket: 1,  mac: "08:00:27:EC:E2:FF", ip: "10.0.0.254"}

  login_nodes:
   - { name: "login-1", vnfs: '{{login_chroot}}', cpus: 8, sockets: 2, corespersocket: 4,  mac: "00:26:b9:2e:21:ed", ip: "10.2.255.137"}
 
  gpu_nodes:
   - { name: "gpu-compute-1", vnfs: '{{gpu_chroot}}', gpus: 4, gpu_type: "gtx_TitanX", cpus: 16, sockets: 2, corespersocket: 8,  mac: "0c:c4:7a:6e:9d:6e", ip: "10.2.255.47"}
 
  viz_nodes:
   - { name: "viz-node-0-0", vnfs: gpu_chroot, gpus: 2, gpu_type: nvidia_gtx_780, cpus: 8, sockets: 2, corespersocket: 4,  mac: "foo", ip: "bar"}


#automatic variables for internal use
# Don't edit these!
  compute_node_glob: "{{ compute_node_prefix }}[0-{{ num_compute_nodes|int - 1}}]"
  node_glob_bash: "{{ compute_node_prefix }}{0..{{ num_compute_nodes|int - 1}}}"
  login_node_glob_bash: "{{ login_node_prefix }}{0..{{ num_login_nodes|int - 1}}}"
