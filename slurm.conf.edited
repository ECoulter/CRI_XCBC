#
# Example slurm.conf file. Please run configurator.html
# (in doc/html) to build a configuration file customized
# for your environment.
#
#
# slurm.conf file generated by configurator.html.
#
# See the slurm.conf man page for more information.
#
ClusterName=divsci-hpcc
ControlMachine=hpcphi
#ControlAddr=
#BackupController=
#BackupAddr=
#
SlurmUser=slurm
SlurmdUser=root
SlurmctldPort=6817
SlurmdPort=6818
AuthType=auth/munge
#JobCredentialPrivateKey=
#JobCredentialPublicCertificate=
StateSaveLocation=/tmp
SlurmdSpoolDir=/tmp/slurmd
SwitchType=switch/none
MpiDefault=none
SlurmctldPidFile=/var/run/slurmctld.pid
SlurmdPidFile=/var/run/slurmd.pid
ProctrackType=proctrack/pgid
#PluginDir=
#FirstJobId=
ReturnToService=1
#MaxJobCount=
#PlugStackConfig=
#PropagatePrioProcess=
#PropagateResourceLimits=
#PropagateResourceLimitsExcept=
#Prolog=
#Epilog=
#SrunProlog=
#SrunEpilog=
#TaskProlog=
#TaskEpilog=
#TaskPlugin=
#TrackWCKey=no
#TreeWidth=50
#TmpFS=
#UsePAM=
#
# TIMERS
SlurmctldTimeout=300
SlurmdTimeout=300
InactiveLimit=0
MinJobAge=300
KillWait=30
Waittime=0
#
# SCHEDULING
SchedulerType=sched/backfill
#SchedulerAuth=
#SchedulerPort=
#SchedulerRootFilter=
SelectType=select/linear
FastSchedule=1
#PriorityType=priority/multifactor
#PriorityDecayHalfLife=14-0
#PriorityUsageResetPeriod=14-0
#PriorityWeightFairshare=100000
#PriorityWeightAge=1000
#PriorityWeightPartition=10000
#PriorityWeightJobSize=1000
#PriorityMaxAge=1-0
#
# LOGGING
SlurmctldDebug=3
SlurmctldLogFile=/var/log/slurmctld.log
SlurmdDebug=3
SlurmdLogFile=/var/log/slurmd.log
JobCompType=jobcomp/none
#JobCompLoc=
#
# ACCOUNTING
#JobAcctGatherType=jobacct_gather/linux
#JobAcctGatherFrequency=30
#
#AccountingStorageType=accounting_storage/slurmdbd
#AccountingStorageHost=
#AccountingStorageLoc=
#AccountingStoragePass=
#AccountingStorageUser=
#
#Partition Priorities
#
SelectType=select/cons_res #Depending on how you want this...
## Could use select/linear instead - ONE JOB PER NODE though.
SelectTypeParameter=CR_CPUS
#DefMemPerCPU=2GB #(?)
#GraceTime=15s # only if PreemptMode=CANCEL
PreemptType=partition/prio
PreemptMode=REQUEUE,GANG
##different values can be added to each partition!!!
## GANG will resume suspended jobs; required for partition/prio
##A value of CANCEL will always cancel the job.
##A value of CHECKPOINT will checkpoint (if possible) or kill low priority jobs.
##Checkpointed jobs are not automatically restarted.
##A value of REQUEUE will requeue (if possible) or kill low priority jobs. Requeued jobs are permitted to be restarted on different resources.
##A value of SUSPEND will suspend and resume jobs. If PreemptType=preempt/partition_prio is configured then a value of SUSPEND will suspend and automatically resume the low priority jobs. If PreemptType=preempt/qos is configured, then the jobs sharing resources will always time slice rather than one job remaining suspended. The SUSPEND option must be used with the GANG option (e.g. "PreemptMode=SUSPEND,GANG").
PriorityTier=1 
## this is a Partition parameter; default is 1 - given like this,
## outside of a partition, sets a default. 
OverSubscribe=FORCE:2
## again; a per-partition setting. Sets an upper limit on # jobs
## per node in a partition.
## observe queue behaviour with squeue -i
#
#
#GENERAL RESOURCE
GresTypes=gpu
#
# COMPUTE NODES
#NodeName=compute-[0-0] Procs=8 State=UNKNOWN
#
NodeName=compute-1-14 Sockets=2 CoresPerSocket=4 State=UNKNOWN
NodeName=compute-1-15 Sockets=2 CoresPerSocket=4 State=UNKNOWN
NodeName=compute-1-16 Sockets=2 CoresPerSocket=4 State=UNKNOWN
NodeName=compute-1-17 Sockets=2 CoresPerSocket=4 State=UNKNOWN
NodeName=compute-1-18 Sockets=2 CoresPerSocket=4 State=UNKNOWN
NodeName=compute-3-10 Sockets=2 CoresPerSocket=4 State=UNKNOWN
# PARTITIONS
PartitionName=test Nodes=compute-1-[14-18] Default=YES MaxTime=INFINITE State=UP
